# -*- coding: utf-8 -*-
"""Content Recommendation System ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cEIMeTcjwRuFBpbmFPwe5UAwNlhn_IB0
"""

!pip install pandas numpy scikit-learn matplotlib seaborn beautifulsoup4 newsapi-python python-dotenv requests
!pip install xgboost
!pip install git+https://github.com/kotartemiy/pygooglenews

import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
from bs4 import BeautifulSoup
import requests
from newsapi import NewsApiClient
import os
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
data = pd.read_csv('/content/influencer_dataset_Modelready.csv')

# Convert post_date to datetime and extract features
data['post_date'] = pd.to_datetime(data['post_date'], format='%d-%m-%Y')

import numpy as np
from sklearn.preprocessing import LabelEncoder

# Calculate engagement score (weighted combination of likes, comments, views)
data['engagement_score'] = (0.5 * data['likes']/data['followers'] +
                          0.3 * data['comments']/data['followers'] +
                          0.2 * (data['views']/data['followers'] if 'views' in data.columns else 0))

# Label content as hit or miss based on engagement score
# Hit = top 25%, Miss = bottom 25%, Neutral = middle 50%
percentiles = data['engagement_score'].quantile([0.25, 0.75]).values
data['performance_label'] = np.where(data['engagement_score'] >= percentiles[1], 'hit',
                                   np.where(data['engagement_score'] <= percentiles[0], 'miss', 'neutral'))
# Feature engineering
data['hashtag_count'] = data['hashtags'].apply(lambda x: len(str(x).split(',')))
data['caption_sentiment'] = data['caption'].apply(lambda x: len(str(x)))

# Encode categorical variables (excluding performance_label as we'll use it as-is for analysis)
label_encoders = {}
categorical_cols = ['content_type', 'platform', 'niche', 'country', 'cluster']
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

# Split data
features = ['likes', 'comments', 'engagement_rate', 'likes_per_follower',
            'caption_length', 'comments_to_likes_ratio', 'post_day', 'post_month', 'post_weekday',
            'content_type', 'platform', 'niche', 'country', 'cluster']  # Add all features here
X = data[features]
y = data['performance_label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
num_cols = ['likes', 'comments', 'engagement_rate', 'likes_per_follower',
            'caption_length', 'comments_to_likes_ratio', 'post_day', 'post_month', 'post_weekday']
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# First encode your labels to numerical values
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Model Training
# Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train_encoded)

# XGBoost Classifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_train, y_train_encoded)

# To get back the original labels when needed:
# y_pred_labels = label_encoder.inverse_transform(y_pred_encoded)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Evaluate models
def evaluate_model(model, X_test, y_test_encoded, label_encoder):
    # Get predictions (encoded)
    y_pred_encoded = model.predict(X_test)

    # Convert both true and predicted labels back to original strings
    y_test = label_encoder.inverse_transform(y_test_encoded)
    y_pred = label_encoder.inverse_transform(y_pred_encoded)

    # Print classification report
    print(classification_report(y_test, y_pred))

    # Create confusion matrix (using encoded labels for correct ordering)
    cm = confusion_matrix(y_test_encoded, y_pred_encoded)

    # Plot with original labels
    labels = label_encoder.classes_
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

print("Random Forest Performance:")
evaluate_model(rf, X_test, y_test_encoded, label_encoder)

print("\nXGBoost Performance:")
evaluate_model(xgb, X_test, y_test_encoded, label_encoder)

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd

# Configuration (no .env needed)
NEWS_API_KEY = "579b8a73fbc64082a358f3dbdf70c8cc"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def scrape_news():
    # Attempt 1: NewsAPI (direct key)
    try:
        if not NEWS_API_KEY:
            raise ValueError("Add your NewsAPI key in the code")

        api_url = f"https://newsapi.org/v2/top-headlines?country=us&apiKey={NEWS_API_KEY}"
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()

        articles = [{
            'title': article['title'],
            'source': article['source']['name'],
            'published_at': article['publishedAt'],
            'url': article['url']
        } for article in response.json()['articles'][:20]]  # Limit to 20 articles

        return pd.DataFrame(articles)

    except Exception as api_error:
        print(f"NewsAPI failed: {api_error}\nFalling back to web scraping...")

    # Attempt 2: Web Scraping (Colab-compatible)
    sources = [
        {'name': 'Google News', 'url': 'https://news.google.com/rss', 'parser': 'rss'},
        {'name': 'BBC', 'url': 'https://feeds.bbci.co.uk/news/rss.xml', 'parser': 'rss'},
        {'name': 'Reuters', 'url': 'https://www.reuters.com/world/us/', 'parser': 'html'}
    ]

    all_articles = []

    for source in sources:
        try:
            print(f"Scraping {source['name']}...")
            response = requests.get(source['url'], headers=HEADERS, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'xml' if source['parser'] == 'rss' else 'html.parser')

            if source['parser'] == 'rss':
                for item in soup.find_all('item')[:10]:
                    all_articles.append({
                        'title': item.title.text,
                        'source': source['name'],
                        'published_at': item.pubDate.text if item.pubDate else datetime.now().strftime('%Y-%m-%d'),
                        'url': item.link.text
                    })
            else:
                for article in soup.find_all('article')[:10]:
                    title = article.find('h3')
                    if title:
                        all_articles.append({
                            'title': title.text.strip(),
                            'source': source['name'],
                            'published_at': datetime.now().strftime('%Y-%m-%d'),
                            'url': article.find('a')['href'] if article.find('a') else ''
                        })

        except Exception as e:
            print(f"Failed to scrape {source['name']}: {str(e)}")
            continue

    return pd.DataFrame(all_articles) if all_articles else pd.DataFrame()

# Run and display
print("Fetching trending news...")
trending_news = scrape_news()

if not trending_news.empty:
    print("\nTop Trending News:")
    display(trending_news[['title', 'source']].head(10))  # Colab-friendly display
else:
    print(" No news fetched. Check errors above.")

def analyze_performance(data):
    # Best performing content types
    content_performance = data.groupby(['content_type', 'performance_label']).size().unstack().fillna(0)
    if 'hit' in content_performance.columns:
        content_performance['hit_rate'] = content_performance['hit'] / (content_performance.sum(axis=1))
    else:
        content_performance['hit_rate'] = 0

    # Best performing niches
    niche_performance = data.groupby(['niche', 'performance_label']).size().unstack().fillna(0)
    if 'hit' in niche_performance.columns:
        niche_performance['hit_rate'] = niche_performance['hit'] / (niche_performance.sum(axis=1))
    else:
        niche_performance['hit_rate'] = 0

    # Best posting times
    time_performance = data.groupby(['post_weekday', 'performance_label']).size().unstack().fillna(0)
    if 'hit' in time_performance.columns:
        time_performance['hit_rate'] = time_performance['hit'] / (time_performance.sum(axis=1))
    else:
        time_performance['hit_rate'] = 0

    return {
        'content_type': content_performance.sort_values('hit_rate', ascending=False),
        'niche': niche_performance.sort_values('hit_rate', ascending=False),
        'posting_time': time_performance.sort_values('hit_rate', ascending=False)
    }

# Perform analysis
performance_analysis = analyze_performance(data)

print("Best Performing Content Types:")
print(performance_analysis['content_type'].head())

print("\nBest Performing Niches:")
print(performance_analysis['niche'].head())

print("\nBest Posting Times (Weekday where 0=Monday, 6=Sunday):")
print(performance_analysis['posting_time'].head())

# Trend Analysis and Content Suggestions
def extract_keywords(news_df, niche):
    # Simple keyword extraction based on niche
    keywords = {
        'Tech': ['tech', 'ai', 'artificial intelligence', 'gadget', 'innovation', 'software', 'app', 'startup'],
        'Gaming': ['game', 'gaming', 'esports', 'console', 'stream', 'twitch', 'tournament'],
        'Food': ['food', 'recipe', 'restaurant', 'chef', 'cooking', 'dining', 'cuisine'],
        'Fitness': ['fitness', 'workout', 'exercise', 'gym', 'health', 'wellness', 'diet'],
        'Fashion': ['fashion', 'style', 'clothing', 'designer', 'trend', 'outfit', 'model' 'coachella' , 'hair'],
        'Travel': ['travel', 'destination', 'vacation', 'hotel', 'flight', 'tourism', 'adventure'],
        'Education': ['education', 'school', 'study', 'learning', 'student', 'university', 'course']
    }

    relevant_keywords = keywords.get(niche, [])
    relevant_articles = []

    for _, article in news_df.iterrows():
        title = article['title'].lower()
        if any(keyword in title for keyword in relevant_keywords):
            relevant_articles.append(article)

    return pd.DataFrame(relevant_articles)

def suggest_content(niche, creator_id=None):
    # Get trending topics for the niche
    niche_news = extract_keywords(trending_news, niche)

    # Get creator's past performance if ID is provided
    creator_insights = None
    if creator_id:
        creator_data = data[data['influencer_id'] == creator_id]
        if not creator_data.empty:
            creator_performance = analyze_performance(creator_data)
            creator_insights = {
                'best_content_types': creator_performance['content_type'].index[:3],
                'best_post_times': creator_performance['posting_time'].index[:3]
            }

    # General niche recommendations
    niche_data = data[data['niche'] == label_encoders['niche'].transform([niche])[0]]
    niche_performance = analyze_performance(niche_data)

    # Get top hashtags in this niche
    top_hashtags = niche_data['hashtags'].value_counts().head(5).index.tolist()

    # Prepare suggestions
    suggestions = {
        'trending_topics': niche_news['title'].tolist()[:5] if not niche_news.empty else ["No trending topics found"],
        'recommended_content_types': [label_encoders['content_type'].inverse_transform([i])[0] for i in niche_performance['content_type'].index[:3]],
        'recommended_post_times': niche_performance['posting_time'].index[:3],
        'top_hashtags': top_hashtags,
        'creator_specific': creator_insights
    }

    return suggestions

def display_recommendations(niche, creator_id=None):
    suggestions = suggest_content(niche, creator_id)

    print("\n=== Content Recommendations ===\n")
    print("Trending topics in", niche + ":")
    if suggestions['trending_topics'][0] != "No trending topics found":
        for i, topic in enumerate(suggestions['trending_topics'], 1):
            print(f"{i}. {topic}")
    else:
        print("No trending topics available currently")

    print("\nRecommended content types [0-Image, 1-Reel, 2-Story, 3-Video]:")
    for i, content_type in enumerate(suggestions['recommended_content_types'], 1):
        print(f"{i}. {content_type}")

    print("\nTop hashtags to use:")
    for i, hashtag in enumerate(suggestions['top_hashtags'], 1):
        print(f"{i}. {hashtag}")

    if suggestions['creator_specific']:
        print("\nPersonalized recommendations based on your past performance:")
        print("Best content types for you:", ", ".join(suggestions['creator_specific']['best_content_types']))
        print("Best posting times for you:", ", ".join(str(t) for t in suggestions['creator_specific']['best_post_times']))

# Optimal Posting Time Analysis
def analyze_posting_times(data, niche=None, creator_id=None):
    if creator_id:
        subset = data[data['influencer_id'] == creator_id]
    elif niche:
        niche_code = label_encoders['niche'].transform([niche])[0]
        subset = data[data['niche'] == niche_code]
    else:
        subset = data

    # Analyze by weekday
    weekday_engagement = subset.groupby('post_weekday')['engagement_score'].mean().sort_values(ascending=False)

    # Analyze by month
    month_engagement = subset.groupby('post_month')['engagement_score'].mean().sort_values(ascending=False)

    return {
        'best_weekdays': weekday_engagement.index.tolist(),
        'best_months': month_engagement.index.tolist(),
        'weekday_engagement': weekday_engagement,
        'month_engagement': month_engagement
    }

# Visualize posting time analysis
def plot_posting_times(analysis):
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    sns.barplot(x=analysis['weekday_engagement'].index, y=analysis['weekday_engagement'].values)
    plt.title('Average Engagement by Weekday')
    plt.xlabel('Weekday (0=Monday, 6=Sunday)')
    plt.ylabel('Engagement Score')

    plt.subplot(1, 2, 2)
    sns.barplot(x=analysis['month_engagement'].index, y=analysis['month_engagement'].values)
    plt.title('Average Engagement by Month')
    plt.xlabel('Month')
    plt.ylabel('Engagement Score')

    plt.tight_layout()
    plt.show()

def get_user_input():
    print("\nWelcome to Content Recommendation System")
    print("Please provide some information to get personalized recommendations")

    # Get the original string labels before encoding
    if hasattr(label_encoders['niche'], 'classes_'):
        niches = label_encoders['niche'].classes_
    else:
        # Fallback in case classes_ isn't available
        niches = data['niche'].unique()

    # Convert to strings if they're numeric
    niche_labels = [str(niche) for niche in niches]
    print("\nAvailable Niches:", ", ".join(niche_labels))

    niche = input("Enter your niche (0-Education , 1-Fashion , 2-Fitness , 3-Food , 4-Gaming , 5-Tech , 6-Travel ): ")
    while niche not in niche_labels:
        print("Invalid niche. Please choose from the list.")
        niche = input("Enter your niche: ")

    creator_id = input("Enter your influencer ID (if available, else press enter): ")
    if creator_id:
        try:
            creator_id = int(creator_id)
            if creator_id not in data['influencer_id'].unique():
                print("ID not found in dataset. Will provide general recommendations.")
                creator_id = None
        except ValueError:
            print("Invalid ID format. Will provide general recommendations.")
            creator_id = None

    return niche, creator_id

def generate_recommendations(niche, creator_id=None):
    print("\nGenerating recommendations...")

    # 1. Get content suggestions
    content_suggestions = suggest_content(niche, creator_id)

    print("\n=== Content Recommendations ===")
    print(f"\nTrending topics in {niche}:")
    for i, topic in enumerate(content_suggestions['trending_topics'][:5], 1):
        print(f"{i}. {topic}")

    print("\nRecommended content types:")
    for i, content_type in enumerate(content_suggestions['recommended_content_types'], 1):
        print(f"{i}. {content_type}")

    print("\nTop hashtags to use:")
    for i, hashtag in enumerate(content_suggestions['top_hashtags'], 1):
        print(f"{i}. {hashtag}")

    # 2. Get posting time recommendations
    time_analysis = analyze_posting_times(data, niche, creator_id)

    print("\n=== Posting Time Recommendations ===")
    print("\nBest weekdays to post (0=Monday, 6=Sunday):")
    print(", ".join(map(str, time_analysis['best_weekdays'][:3])))

    print("\nBest months to post:")
    print(", ".join(map(str, time_analysis['best_months'][:3])))

    # 3. Show creator-specific insights if available
    if content_suggestions['creator_specific']:
        print("\n=== Your Personal Performance Insights ===")
        print("\nYour best performing content types:")
        for i, ct in enumerate(content_suggestions['creator_specific']['best_content_types'], 1):
            print(f"{i}. {label_encoders['content_type'].inverse_transform([ct])[0]}")

        print("\nYour best posting times (weekday):")
        print(", ".join(map(str, content_suggestions['creator_specific']['best_post_times'])))

    # Visualize posting time analysis
    plot_posting_times(time_analysis)

if __name__ == "__main__":
    niche, creator_id = get_user_input()
    generate_recommendations(niche, creator_id)

    # predict_new = input("\nWould you like to predict performance for new content? (yes/no): ").lower()
    # if predict_new == 'yes':
    #     print("\nEnter details of your proposed content:")

    #     try:
    #         content_type = input(f"Content type ({', '.join(label_encoders['content_type'].classes_)}): ")
    #         caption = input("Caption: ")
    #         hashtags = input("Hashtags (comma separated): ")
    #         planned_weekday = int(input("Planned posting weekday (0-6): "))
    #         planned_month = int(input("Planned posting month (1-12): "))

    #         new_content = {
    #             'likes': data['likes'].median(),
    #             'comments': data['comments'].median(),
    #             'engagement_rate': data['engagement_rate'].median(),
    #             'likes_per_follower': data['likes_per_follower'].median(),
    #             'caption_length': len(caption),
    #             'comments_to_likes_ratio': data['comments_to_likes_ratio'].median(),
    #             'hashtag_count': len(hashtags.split(',')) if hashtags else 0,
    #             'caption_sentiment': len(caption),  # Note: This should be sentiment score, not length
    #             'post_day': datetime.now().day,
    #             'post_month': planned_month,
    #             'post_weekday': planned_weekday,
    #             'content_type': label_encoders['content_type'].transform([content_type])[0],
    #             'platform': data['platform'].mode()[0],
    #             'niche': label_encoders['niche'].transform([niche])[0],
    #             'country': data['country'].mode()[0],
    #             'cluster': data['cluster'].mode()[0]
    #         }

    #         new_df = pd.DataFrame([new_content])
    #         new_df[num_cols] = scaler.transform(new_df[num_cols])  # Fix typo: "scaler" not "scaler"
    #         prediction = xgb.predict(new_df[features])
    #         proba = xgb.predict_proba(new_df[features])

    #         print(f"\nPredicted Performance: {label_encoders['performance'].inverse_transform(prediction)[0]}")
    #         print("Prediction Probabilities:")
    #         for i, cls in enumerate(label_encoders['performance'].classes_):
    #             print(f"{cls}: {proba[0][i]:.2f}")
    #     except Exception as e:
    #         print(f"Error: {e}")